from abc import ABC, abstractmethod
import openai
import anthropic
from decouple import config


class AIScorer(ABC):
    prompt = (
        "You are an AI assistant trained to identify errors, contradictions, "
        "and inconsistencies in research paper abstracts. Your task is to analyze "
        "the provided abstract and assign a score from 1 to 100, where 1 indicates "
        "no issues and 100 indicates severe errors or contradictions that completely "
        "undermine the credibility of the research.\n\n"
        "Focus on the following aspects when scoring the abstract:\n"
        "- Logical inconsistencies or contradictions within the abstract\n"
        "- Factual errors or incorrect statements\n"
        "- Inconsistent or conflicting results or conclusions\n"
        "- Unclear or ambiguous language that hinders understanding\n"
        "- Obvious typographical or grammatical errors that impact clarity\n\n"
        "Provide a brief explanation for your score, highlighting the specific errors "
        "or contradictions you identified in the abstract. If you find no significant "
        "issues, assign a low score and explain why the abstract appears to be sound."
    )

    @abstractmethod
    def score_paper(self, abstract):
        pass


def test_score(title, abstract):
    """
    Test the scoring of a paper's title and abstract using the configured AI scorer.

    :param title: The title of the paper.
    :param abstract: The abstract of the paper.
    :return: A tuple containing the score and explanation.
    """
    # Initialize the AI scorer with the correct API key from environment variables
    scorer_type = config('SCORER_TYPE', default='OpenAI')
    api_key = config('SCORER_API_KEY')
    if scorer_type == 'OpenAI':
        scorer = OpenAIScorer(api_key=api_key)
    elif scorer_type == 'Anthropic':
        scorer = AnthropicScorer(api_key=api_key)
    else:
        raise ValueError(f"Invalid scorer type: {scorer_type}")

    # Combine the title and abstract for scoring
    full_text = f"Title: {title}\n\nAbstract:\n{abstract}"
    score, explanation = scorer.score_paper(full_text)
    return score, explanation


class OpenAIScorer(AIScorer):
    def __init__(self, api_key):
        self.api_key = api_key

    def score_paper(self, abstract):
        score = openai.Completion.create(
            engine="davinci",
            prompt=self.prompt + "\n\nAbstract:\n" + abstract,
            max_tokens=150,
            api_key=self.api_key
        )
        explanation = "Generated by OpenAI's GPT-3 model."
        return (score['choices'][0]['text'].strip(), explanation)


class AnthropicScorer(AIScorer):
    def __init__(self, api_key, prompt=None):
        self.api_key = api_key
        self.prompt = AIScorer.prompt  # Use the base class prompt by default

    def score_paper(self, abstract):
        # Placeholder for Anthropic API call, as the actual API details are not provided
        # This should be replaced with the actual API call when the details are available
        # Placeholder for Anthropic API call
        score = 50  # Dummy score for the placeholder
        explanation = "Placeholder explanation for Anthropic API."
        return (score, explanation)
